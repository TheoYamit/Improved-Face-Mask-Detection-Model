{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First dealing with all of the imports\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras as keras\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import to_categorical\n",
    "from keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and Preprocessing the Dataset \n",
    "import xml.etree.ElementTree as ElemTree\n",
    "# Images and labels list\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "annotation_path = 'dataset1/annotations'\n",
    "images_path = 'dataset1/images'\n",
    "\n",
    "# Function to parse the xml file \n",
    "def parse_xml(xml_file):\n",
    "    tree = ElemTree.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    bounding_boxes = []\n",
    "    temp_labels = []\n",
    "\n",
    "    for object in root.findall('object'):\n",
    "        temp_labels.append(object[0].text)\n",
    "        xmin = int(object[5][0].text)\n",
    "        ymin = int(object[5][1].text)\n",
    "        xmax = int(object[5][2].text)\n",
    "        ymax = int(object[5][3].text)\n",
    "        bounding_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return bounding_boxes, temp_labels\n",
    "\n",
    "# Looping through the annotations file directory \n",
    "for xml_file in os.listdir(annotation_path):\n",
    "    if xml_file.endswith(\".xml\"):\n",
    "        image_path = os.path.join(images_path, xml_file[:-4] + \".png\")\n",
    "        xml_path = os.path.join(annotation_path, xml_file)\n",
    "\n",
    "        image = cv2.imread(image_path)\n",
    "        bounding_boxes, xml_labels = parse_xml(xml_path)\n",
    "\n",
    "        for bound_box, lab in zip(bounding_boxes, xml_labels):\n",
    "            xmin, ymin, xmax, ymax = bound_box\n",
    "            roi = image[ymin:ymax, xmin:xmax]\n",
    "            roi = cv2.resize(roi, (400, 400))\n",
    "            roi = img_to_array(roi)\n",
    "            roi = preprocess_input(roi)\n",
    "            images.append(roi)\n",
    "            labels.append(lab)\n",
    "\n",
    "images = np.array(images, dtype=\"float32\")\n",
    "labels = np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the labels (with mask, no mask, wearing a mask incorrectly)\n",
    "lb = LabelBinarizer()\n",
    "labels = lb.fit_transform(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "print(labels.shape)\n",
    "print(lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "(train_images, test_images, train_labels, test_labels) = train_test_split(images, labels, test_size = 0.30, stratify=labels, random_state = 42)\n",
    "# Test size of 0.20 means that 20% of data is for the test, and the rest is for the training set. \n",
    "# Picked a ranom state for reproduciablity if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation step.\n",
    "# Not entirely needed but helps to increase the diversity of the dataset\n",
    "data_aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = data_aug.flow(train_images, train_labels, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_focal_crossentropy\n",
    "\n",
    "\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(400, 400, 3))\n",
    "\n",
    "# Freeze base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "predictions = Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Create full model \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss=binary_focal_crossentropy,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "import math\n",
    "\n",
    "# Step decay schedule \n",
    "def step_decay(epoch):\n",
    "   initial_lrate = 0.1\n",
    "   drop = 0.5\n",
    "   epochs_drop = 10.0\n",
    "   lrate = initial_lrate * math.pow(drop,  \n",
    "           math.floor((1+epoch)/epochs_drop))\n",
    "   return lrate\n",
    "# Learning rate scheduler callback\n",
    "lr_scheduler = LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks=[lr_scheduler]\n",
    "\n",
    "model.fit(\n",
    "  train_images, \n",
    "  train_labels,\n",
    "  epochs=50,\n",
    "  batch_size=2,\n",
    "  validation_data=(test_images, test_labels),\n",
    "  callbacks=[lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "mask_model = load_model('model4')\n",
    "\n",
    "# Colors for boxes\n",
    "colors = [(0,255,0),(0,0,255),(255,0,0)] \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  faces = face_cascade.detectMultiScale(frame)\n",
    "\n",
    "  for (x,y,w,h) in faces:\n",
    "\n",
    "    # Preprocess face\n",
    "    face_img = frame[y:y+h, x:x+w] \n",
    "    face_img = cv2.resize(face_img, (224,224))\n",
    "    face_img = preprocess(face_img)  \n",
    "\n",
    "    # Predict\n",
    "    preds = mask_model.predict(face_img)[0]\n",
    "    print(preds)\n",
    "    pred_class = np.argmax(preds)\n",
    "    print(pred_class)\n",
    "    pred_label = lb.inverse_transform()\n",
    "\n",
    "\n",
    "    # Draw box\n",
    "    color = colors[pred_class]\n",
    "    cv2.rectangle(frame, (x,y), (x+w,y+h), color, 2)\n",
    "\n",
    "    # Annotate\n",
    "    label = f\"Prediction: {pred_label}\"\n",
    "    cv2.putText(frame, label, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "  cv2.imshow('Video', frame)\n",
    "\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "cap.release()  \n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
